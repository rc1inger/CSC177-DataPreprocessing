{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessing Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and rename columns for clarity\n",
    "data = pd.read_csv('data/heart_disease.csv')\n",
    "data.columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Duplicates, Handle Missing Values, and Remove Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'disease' column as it's the target variable for prediction\n",
    "data = data.drop(['disease'], axis=1)\n",
    "\n",
    "# Print the shape of the dataset to understand its dimensions\n",
    "print(f'Number of instances = {{data.shape[0]}}')  # Number of rows\n",
    "print(f'Number of attributes = {{data.shape[1]}}')  # Number of columns\n",
    "print(data.head())  # Display the first few rows for a quick overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Modularization: Created a function for duplicate removal**\n",
    "def remove_duplicates(df):\n",
    "    dups = df.duplicated()\n",
    "    print(f'Number of duplicate rows = {{dups.sum()}}')  # Count duplicates\n",
    "    df_cleaned = df.drop_duplicates()\n",
    "    print(f'Number of instances after dropping duplicates = {{df_cleaned.shape[0]}}')  # New row count\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the dataset\n",
    "data2 = remove_duplicates(data)\n",
    "\n",
    "# Replace '?' with NaN to handle missing values properly\n",
    "data3 = data2.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Handle missing values**\n",
    "def handle_missing_values(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    print('Replace missing values with median')\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].median())  # No inplace, reassign the column directly\n",
    "    return df\n",
    "\n",
    "# Handle missing values in the dataset\n",
    "data3 = handle_missing_values(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Visualize missing values**\n",
    "def visualize_missing_values(df):\n",
    "    missing_values = df.isna().sum()\n",
    "    missing_values = missing_values[missing_values > 0]\n",
    "    if not missing_values.empty:\n",
    "        missing_values.plot(kind='bar', figsize=(10, 5))  # Plot only columns with missing values\n",
    "        plt.title('Missing Values Count')  # Title for the plot\n",
    "        plt.xlabel('Features')  # X-axis label\n",
    "        plt.ylabel('Count')  # Y-axis label\n",
    "        plt.show()  # Display the plot\n",
    "    else:\n",
    "        print(\"No missing values to visualize.\")\n",
    "\n",
    "# Visualize missing values after handling missing data\n",
    "visualize_missing_values(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Visualizing potential outliers using boxplots**\n",
    "data3.boxplot(figsize=(20, 3))  # Initial boxplot for visual inspection\n",
    "plt.title('Boxplot of Features Before Outlier Removal')\n",
    "plt.show()  # Display the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Remove outliers**\n",
    "def remove_outliers(df):\n",
    "    Z = (df - df.mean()) / df.std()  # Z-score normalization\n",
    "    print(f'Number of rows before removing outliers = {{Z.shape[0]}}')  # Initial row count\n",
    "    Z2 = df.loc[((Z > -3).sum(axis=1) == len(df.columns)) & ((Z <= 3).sum(axis=1) == len(df.columns)), :]\n",
    "    print(f'Number of rows after removing outliers = {{Z2.shape[0]}}')  # Count after outlier removal\n",
    "    return Z2\n",
    "\n",
    "# Remove outliers from the dataset\n",
    "data3 = remove_outliers(data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One hot Encoding and Feature Scaling on the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Label Encoding for binary columns and One-Hot Encoding for multiclass columns\n",
    "def encode_categorical(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    # Label encode binary categorical columns\n",
    "    df['sex'] = le.fit_transform(df['sex'])\n",
    "    df['fbs'] = le.fit_transform(df['fbs'])\n",
    "    df['exang'] = le.fit_transform(df['exang'])\n",
    "    \n",
    "    # One-Hot encode multiclass columns\n",
    "    df = pd.get_dummies(df, columns=['cp', 'restecg', 'slope', 'ca', 'thal'])\n",
    "    return df\n",
    "\n",
    "X_encoded = encode_categorical(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Feature scaling**\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data3_scaled = pd.DataFrame(scaler.fit_transform(data3), columns=data3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Visualize cleaned data**\n",
    "def visualize_data(df):\n",
    "    df.boxplot(figsize=(20, 3))  # Size of the plot\n",
    "    plt.title('Boxplot of Features After Cleaning and Scaling')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cleaned dataset\n",
    "visualize_data(data3_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the Dataset 80% train, 20% test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Split dataset**\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets (80-20 split)\n",
    "train_data, test_data = train_test_split(data3_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# **Calculate mean and standard deviation for both sets**\n",
    "train_mean = train_data.mean()\n",
    "train_std = train_data.std()\n",
    "\n",
    "test_mean = test_data.mean()\n",
    "test_std = test_data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mean and standard deviation for training and test sets\n",
    "print(f'Train Mean:\\n{train_mean}')\n",
    "print(f'Train Std:\\n{train_std}')\n",
    "print(f'Test Mean:\\n{test_mean}')\n",
    "print(f'Test Std:\\n{test_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the training and test values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the difference between train and test means\n",
    "mean_diff = abs(train_mean - test_mean)\n",
    "std_diff = abs(train_std - test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean Differences Between Train and Test:\\n{mean_diff}\\n')\n",
    "print(f'Standard Deviation Differences Between Train and Test:\\n{std_diff}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the differences\n",
    "if mean_diff.mean() < 0.1 and std_diff.mean() < 0.1:\n",
    "    print(\"The training and test sets are well-balanced and represent similar distributions.\")\n",
    "else:\n",
    "    print(\"There are some differences between the training and test sets. Further analysis might be required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
